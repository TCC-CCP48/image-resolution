{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8f1f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 20:25:14.549338: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-23 20:25:14.549439: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Imports necess√°rios\n",
    "import os\n",
    "import PIL\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca2a4489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['img20210515_17195465.jpg',\n",
       " 'img20210515_16403543.jpg',\n",
       " 'img20210515_16385031.jpg',\n",
       " '45.jpg',\n",
       " 'img20210515_15373823.jpg',\n",
       " 'img20210515_17374084.jpg',\n",
       " 'img20210515_16464084.jpg',\n",
       " 'img20210515_16425192.jpg',\n",
       " 'img20210515_17472074.jpg',\n",
       " 'img20210515_16360937.jpg',\n",
       " 'img20210515_15595523.jpg',\n",
       " 'img20210515_16040522.jpg',\n",
       " 'img20210515_17253306.jpg',\n",
       " 'img20210515_16482898.jpg',\n",
       " 'img20210515_17571504.jpg',\n",
       " 'img20210515_15540198.jpg',\n",
       " 'img20210515_15351416.jpg',\n",
       " 'img20210515_16374742.jpg',\n",
       " 'img20210515_17311878.jpg',\n",
       " 'img20210515_15584433.jpg',\n",
       " 'img20210515_16075108.jpg',\n",
       " 'img20210515_15570053.jpg',\n",
       " 'img20210515_17043668.jpg',\n",
       " 'img20210515_16501795.jpg',\n",
       " 'img20210515_16005061.jpg',\n",
       " '5.jpg',\n",
       " '85.jpg',\n",
       " 'img20210515_16022920.jpg',\n",
       " 'img20210515_16102331.jpg',\n",
       " '12.jpg',\n",
       " 'img20210515_16394287.jpg',\n",
       " 'img20210515_16513647.jpg',\n",
       " '1.jpg',\n",
       " 'img20210515_17505084.jpg',\n",
       " 'img20210515_17053501.jpg',\n",
       " 'img20210515_16565564.jpg',\n",
       " '8575.jpg',\n",
       " 'img20210515_17013073.jpg',\n",
       " 'img20210515_16584993.jpg',\n",
       " 'img20210515_15384297.jpg',\n",
       " 'img20210515_16551695.jpg',\n",
       " 'img20210515_16134309.jpg',\n",
       " 'img20210515_16091987.jpg',\n",
       " '24.jpg',\n",
       " 'img20210515_17224878.jpg',\n",
       " 'img20210515_16415748.jpg',\n",
       " 'img20210515_17384906.jpg',\n",
       " '2453.jpg',\n",
       " '41.jpg',\n",
       " 'img20210515_16531062.jpg',\n",
       " 'img20210515_16120268.jpg',\n",
       " 'img20210515_15324150.jpg',\n",
       " '4.jpg',\n",
       " 'img20210515_16452011.jpg',\n",
       " 'img20210515_15552859.jpg',\n",
       " 'img20210515_17213657.jpg',\n",
       " 'img20210515_17001475.jpg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pasta com imgs\n",
    "imgs: str = r\"/home/dangelo/Downloads/imgs_teste\"\n",
    "os.listdir(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef6de8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cc861ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "contador = 1\n",
    "\n",
    "for img in os.listdir(imgs):\n",
    "    caminho_total = os.path.join(imgs, img)\n",
    "    novo_caminho = os.path.join(imgs, f\"img_{contador}_lr.jpg\")\n",
    "    os.rename(caminho_total, novo_caminho)\n",
    "    contador += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a09ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "688dfb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constroi_modelo(upscale_factor=3, channels=1):\n",
    "    conv_args = {\n",
    "        \"activation\": \"relu\",\n",
    "        \"kernel_initializer\": \"Orthogonal\",\n",
    "        \"padding\": \"same\",\n",
    "    }\n",
    "    inputs = keras.Input(shape=(None, None, channels))\n",
    "    x = layers.Conv2D(64, 5, **conv_args)(inputs)\n",
    "    x = layers.Conv2D(64, 3, **conv_args)(x)\n",
    "    x = layers.Conv2D(32, 3, **conv_args)(x)\n",
    "    x = layers.Conv2D(channels * (upscale_factor ** 2), 3, **conv_args)(x)\n",
    "    outputs = tf.nn.depth_to_space(x, upscale_factor)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    loss_fn = keras.losses.MeanSquaredError()\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss=loss_fn,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def conversor_alta_resolucao(modelo, img):\n",
    "    ycbcr = img.convert(\"YCbCr\")\n",
    "    img_cinza, canal_cb, canal_cr = ycbcr.split()\n",
    "    img_cinza = img_to_array(img_cinza)\n",
    "    img_cinza = img_cinza.astype(\"float32\") / 255.0\n",
    "\n",
    "    entrada = np.expand_dims(img_cinza, axis=0)\n",
    "    saida = modelo.predict(entrada)\n",
    "\n",
    "    saida_img_cinza = saida[0]\n",
    "    saida_img_cinza *= 255.0\n",
    "\n",
    "    saida_img_cinza = saida_img_cinza.clip(0, 255)\n",
    "    saida_img_cinza = saida_img_cinza.reshape(\n",
    "        (np.shape(saida_img_cinza)[0], \n",
    "         np.shape(saida_img_cinza)[1])\n",
    "    )\n",
    "\n",
    "    saida_img_cinza = PIL.Image.fromarray(np.uint8(saida_img_cinza), mode=\"L\")\n",
    "    saida_canal_cb = canal_cb.resize(saida_img_cinza.size, PIL.Image.BICUBIC)\n",
    "    saida_canal_cr = canal_cr.resize(saida_img_cinza.size, PIL.Image.BICUBIC)\n",
    "    img_resultado = PIL.Image.merge(\"YCbCr\", \n",
    "            (saida_img_cinza, saida_canal_cb, saida_canal_cr)).convert(\n",
    "        \"RGB\"\n",
    "    )\n",
    "    return img_resultado\n",
    "\n",
    "\n",
    "def abreImagem(nome):\n",
    "    return PIL.Image.open(nome)\n",
    "\n",
    "\n",
    "def printImagem(modelo, nome):\n",
    "    imgBaixa = abreImagem(nome)\n",
    "    display(imgBaixa)\n",
    "    \n",
    "    imgAlta = conversor_alta_resolucao(modelo, imgBaixa)\n",
    "    display(imgAlta)\n",
    "    \n",
    "    print(f\"Imagem baixa: {imgBaixa.size[0], imgBaixa.size[1]}\")\n",
    "    print(f\"Imagem alta: {imgAlta.size[0], imgAlta.size[1]}\")\n",
    "    \n",
    "\n",
    "# Pasta com os checkpoints\n",
    "PASTA_CHECKPOINT: str = r\"/home/dangelo/dev/TCC/image-resolution/src/criacao_modelo/checkpoint_100_epocas\"\n",
    "ARQUIVO_CHECKPOINT: str = os.path.join(PASTA_CHECKPOINT, \"meu-checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06949abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, None, 1)]   0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, None, None, 64)    1664      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, None, None, 32)    18464     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, None, None, 9)     2601      \n",
      "_________________________________________________________________\n",
      "tf.nn.depth_to_space (TFOpLa (None, None, None, 1)     0         \n",
      "=================================================================\n",
      "Total params: 59,657\n",
      "Trainable params: 59,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 20:31:57.557440: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-08-23 20:31:57.557466: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-08-23 20:31:57.557495: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dangelo-mint): /proc/driver/nvidia/version does not exist\n",
      "2021-08-23 20:31:57.557764: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Construindo o modelo\n",
    "modelo = constroi_modelo(upscale_factor=3, channels=1)\n",
    "modelo.load_weights(ARQUIVO_CHECKPOINT)\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "890c5173",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in lista_imagens:\n",
    "    caminho_img = os.path.join(imgs, img)\n",
    "    imagem_lr = abreImagem(caminho_img)\n",
    "    \n",
    "    \n",
    "    caminho_img_hr = caminho_img.replace(\"_lr\", \"_hr\")\n",
    "    imagem_hr = conversor_alta_resolucao(modelo, imagem_lr)\n",
    "    \n",
    "    imagem_hr.save(caminho_img_hr)\n",
    "    \n",
    "    contador += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2075c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
