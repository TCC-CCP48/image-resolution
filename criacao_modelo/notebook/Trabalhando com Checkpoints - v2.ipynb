{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda059a9",
   "metadata": {},
   "source": [
    "## Objetivo: criar um script para treino do modelo via checkpoint, já determinando o número de épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c36752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-05 18:29:28.289953: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-05 18:29:28.289979: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PIL\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ed4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicão de classes e funções importantes para o programa\n",
    "PASTA_CHECKPOINT: str = \"./checkpoint/\"\n",
    "ARQUIVO_CHECKPOINT: str = \"./checkpoint/meu-checkpoint\"\n",
    "\n",
    "\n",
    "##################################  CLASSES E FUNÇÕES UTILIZADAS  #################################\n",
    "\n",
    "class ModeloCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, fator_escala, caminho_imagens_teste):\n",
    "        super(ModeloCallback, self).__init__()\n",
    "        self.img_teste = coleta_img_baixa_resolucao(\n",
    "            load_img(caminho_imagens_teste[0]), fator_escala\n",
    "        )\n",
    "\n",
    "    # Método herdado de keras.callbacks.Callback\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.psnr = []\n",
    "\n",
    "    # Método herdado de keras.callbacks.Callback\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"Média da métrica PSNR por época: %.2f\" % (np.mean(self.psnr)))\n",
    "\n",
    "    # Método herdado de keras.callbacks.Callback\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        self.psnr.append(10 * math.log10(1 / logs[\"loss\"]))\n",
    "\n",
    "\n",
    "\n",
    "def redimensionar(array_img):\n",
    "    return array_img / 255\n",
    "\n",
    "\n",
    "def processa_entrada(imagem, tamanho_entrada):\n",
    "    return tf.image.resize(imagem, [tamanho_entrada, tamanho_entrada], method=\"area\")\n",
    "\n",
    "\n",
    "def coleta_img_baixa_resolucao(img, fator):\n",
    "    return img.resize(\n",
    "        (img.size[0] // fator, img.size[1] // fator),\n",
    "        PIL.Image.BICUBIC,\n",
    "    )\n",
    "\n",
    "\n",
    "def constroi_modelo(upscale_factor=3, channels=1):\n",
    "    conv_args = {\n",
    "        \"activation\": \"relu\",\n",
    "        \"kernel_initializer\": \"Orthogonal\",\n",
    "        \"padding\": \"same\",\n",
    "    }\n",
    "    inputs = keras.Input(shape=(None, None, channels))\n",
    "    x = layers.Conv2D(64, 5, **conv_args)(inputs)\n",
    "    x = layers.Conv2D(64, 3, **conv_args)(x)\n",
    "    x = layers.Conv2D(32, 3, **conv_args)(x)\n",
    "    x = layers.Conv2D(channels * (upscale_factor ** 2), 3, **conv_args)(x)\n",
    "    outputs = tf.nn.depth_to_space(x, upscale_factor)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    loss_fn = keras.losses.MeanSquaredError()\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss=loss_fn,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def conversor_alta_resolucao(modelo, img):\n",
    "    ycbcr = img.convert(\"YCbCr\")\n",
    "    img_cinza, canal_cb, canal_cr = ycbcr.split()\n",
    "    img_cinza = img_to_array(img_cinza)\n",
    "    img_cinza = img_cinza.astype(\"float32\") / 255.0\n",
    "\n",
    "    entrada = np.expand_dims(img_cinza, axis=0)\n",
    "    saida = modelo.predict(entrada)\n",
    "\n",
    "    saida_img_cinza = saida[0]\n",
    "    saida_img_cinza *= 255.0\n",
    "\n",
    "    saida_img_cinza = saida_img_cinza.clip(0, 255)\n",
    "    saida_img_cinza = saida_img_cinza.reshape(\n",
    "        (np.shape(saida_img_cinza)[0], \n",
    "         np.shape(saida_img_cinza)[1])\n",
    "    )\n",
    "\n",
    "    saida_img_cinza = PIL.Image.fromarray(np.uint8(saida_img_cinza), mode=\"L\")\n",
    "    saida_canal_cb = canal_cb.resize(saida_img_cinza.size, PIL.Image.BICUBIC)\n",
    "    saida_canal_cr = canal_cr.resize(saida_img_cinza.size, PIL.Image.BICUBIC)\n",
    "    img_resultado = PIL.Image.merge(\"YCbCr\", \n",
    "            (saida_img_cinza, saida_canal_cb, saida_canal_cr)).convert(\n",
    "        \"RGB\"\n",
    "    )\n",
    "    return img_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d093386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################  SCRIPT PARA TREINO DO MODELO  ################################\n",
    "\n",
    "def treino(modelo, pastaCheckpoint):\n",
    "    \n",
    "    # ATENÇÃO: ALTERAR O CAMINHO ABAIXO PARA AS RESPECTIVAS PASTAS\n",
    "    #   Organização das pastas:\n",
    "    #   dir_principal\n",
    "    #       |\n",
    "    #       |--------treino\n",
    "    #       |           |\n",
    "    #       |           |--- imgtreino0.png\n",
    "    #       |           |---      ...\n",
    "    #       |           |--- imgtreino7999.png\n",
    "    #       |\n",
    "    #       |--------validacao\n",
    "    #       |           |\n",
    "    #       |           |--- imgvalidacao0.png\n",
    "    #       |           |---     ...\n",
    "    #       |           |--- imgvalidacao999.png\n",
    "    #       |\n",
    "    #       |--------teste\n",
    "    #       |           |\n",
    "    #       |           |--- imgteste0.png\n",
    "    #       |           |---      ...\n",
    "    #       |           |--- imgteste999.png\n",
    "    #\n",
    "    dir_principal = r\"/home/dangelo/dev/TCC/image-resolution/images\"\n",
    "\n",
    "    # Definição de algumas constantes usadas no programa\n",
    "    TAMANHO_RECORTE: int = 300\n",
    "    FATOR_UPSCALE: int = 3\n",
    "    TAMANHO_IMAGEM_ENTRADA = TAMANHO_RECORTE // FATOR_UPSCALE\n",
    "    LOTE: int = 8\n",
    "    EPOCAS: int = 34\n",
    "    PORCENTAGEM_VALIDACAO: float = 0.1\n",
    "    ARQUIVO_CHECKPOINT: str = os.path.join(pastaCheckpoint, \"meu-checkpoint\")\n",
    "\n",
    "    # Criação dos datasets: treino e validação\n",
    "    ds_treino = image_dataset_from_directory(\n",
    "        directory=dir_principal,\n",
    "        batch_size=LOTE,\n",
    "        image_size=(TAMANHO_RECORTE, TAMANHO_RECORTE),\n",
    "        validation_split=PORCENTAGEM_VALIDACAO,\n",
    "        color_mode=\"grayscale\",\n",
    "        subset=\"training\",\n",
    "        seed=1337,\n",
    "        label_mode=None\n",
    "    )\n",
    "\n",
    "    ds_validacao = image_dataset_from_directory(\n",
    "        directory=dir_principal,\n",
    "        batch_size=LOTE,\n",
    "        image_size=(TAMANHO_RECORTE, TAMANHO_RECORTE),\n",
    "        validation_split=PORCENTAGEM_VALIDACAO,\n",
    "        color_mode=\"grayscale\",\n",
    "        subset=\"validation\",\n",
    "        seed=1337,\n",
    "        label_mode=None\n",
    "    )\n",
    "\n",
    "    ds_treino = ds_treino.map(redimensionar)\n",
    "    ds_validacao = ds_validacao.map(redimensionar)\n",
    "\n",
    "    # Recuperar todos os arquivos para teste\n",
    "    imagens_teste = os.path.abspath(os.path.join(dir_principal, \"test\"))\n",
    "\n",
    "    caminho_imagens_teste: list = []\n",
    "    for imgs in os.listdir(imagens_teste):\n",
    "        final_caminho: str = os.path.join(imagens_teste, imgs)\n",
    "        caminho_imagens_teste.append(final_caminho)\n",
    "\n",
    "    ds_treino = ds_treino.map(\n",
    "            lambda x: (processa_entrada(x, TAMANHO_IMAGEM_ENTRADA), x)\n",
    "    )\n",
    "    ds_treino = ds_treino.prefetch(buffer_size=32)\n",
    "\n",
    "    ds_validacao = ds_validacao.map(\n",
    "            lambda x: (processa_entrada(x, TAMANHO_IMAGEM_ENTRADA), x)\n",
    "    )\n",
    "    ds_validacao = ds_validacao.prefetch(buffer_size=32)\n",
    "\n",
    "    callback_parada_eficiencia = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=10)\n",
    "\n",
    "    callback_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=pastaCheckpoint,\n",
    "        save_weights_only=True,\n",
    "        monitor=\"loss\",\n",
    "        mode=\"min\",\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    callbacks = [ModeloCallback(FATOR_UPSCALE, caminho_imagens_teste), \n",
    "                 callback_parada_eficiencia, \n",
    "                 callback_checkpoint\n",
    "                ]\n",
    "\n",
    "    modelo.fit(\n",
    "        ds_treino, epochs=EPOCAS, callbacks=callbacks, validation_data=ds_validacao, verbose=2\n",
    "    )\n",
    "\n",
    "    modelo.save_weights(ARQUIVO_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a92b32af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 files belonging to 1 classes.\n",
      "Using 900 files for training.\n",
      "Found 1000 files belonging to 1 classes.\n",
      "Using 100 files for validation.\n",
      "Epoch 1/2\n",
      "113/113 - 49s - loss: 0.0155 - val_loss: 0.0022\n",
      "Média da métrica PSNR por época: 26.63\n",
      "Epoch 2/2\n",
      "113/113 - 52s - loss: 0.0017 - val_loss: 0.0014\n",
      "Média da métrica PSNR por época: 28.36\n"
     ]
    }
   ],
   "source": [
    "# Construindo modelo zerado\n",
    "modelo = constroi_modelo(3, 1)\n",
    "\n",
    "try:\n",
    "    # Criando pasta para os checkpoints\n",
    "    if not os.path.exists(PASTA_CHECKPOINT):\n",
    "        os.mkdir(PASTA_CHECKPOINT)\n",
    "\n",
    "    if os.path.exists(ARQUIVO_CHECKPOINT):\n",
    "        modelo.load_weights(ARQUIVO_CHECKPOINT)\n",
    "    \n",
    "    treino(modelo, PASTA_CHECKPOINT)\n",
    "\n",
    "except FileExistsError:\n",
    "    print(\"Arquivo já existe!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
