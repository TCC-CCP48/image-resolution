#### Efficient Sub-Pixel Convolutional Neural Network (ESPCN)

“[...] a new architecture of Convolutional Neural Networks where feature maps are extracted in Low Resolution space. In addition, we introduce an efficient subpixel convolution layer that learns a series of scaling filters to increase the final Low Res feature maps in the High Res output. In doing so, we have effectively replaced the handcrafted bicubic filter in the SuperRes pipeline with more complex scaling filters, trained specifically for each feature map, while reducing the computational complexity of the overall SR operation.” (SHI, 2016)

In digital images, pixels are connected to other pixels, but microscopically, between the physical pixels, there are several tiny pixels, these are called sub-pixels. “The sub-pixel convolution gets its name from the imaginary sub-pixels with fractional indices filled in between the original pixels” (CABALLERO, 2016). This convolution involves two fundamental processes: a general convolutional operation followed by pixel rearrangement. The output channel of the last layer should be C × r × r so that the total number of pixels is consistent with the high resolution image to be obtained. The ESPCN model proposes the creation of a 4-layer model, with the first 3 normal convolution layers and the last, the sub-pixel convolutional layer.

![ESPCN MODEL](https://drive.google.com/uc?id=1zYpuMdiB_WrKww0r7GRm9Ymtg_toJZW9)

First, the model receives the input image in low resolution I LR, where I LR = H × W × C, where I represents the input image and LR indicates that the image is in low resolution, low resolution, H represents the height of the image, height, W represents its width, width, and C represents the number of color channels, for example, the number of color channels would be 3 if the image was in RGB, and passes it to the first convolution layer .

The first layer can be represented by f 1(I LR ; W1, b1) =φ(W1 ∗ I LR + b1), where f 1(I LR ; W1, b1) demonstrates the first layer receiving the image in low resolution, the weight and bias, respectively, and φ(W1 ∗ I LR + b1) demonstrate the layer by applying the weight to the image and then adding the bias and applying the activation function to the result. By convention the first layer applies a convolution of 64 filters and kernel size 5 × 5, followed by a tanh activation layer.

The second and third layers can be represented by f l(I LR ; W1:l, b1:l) = φ(Wl ∗ f l−1 (I LR ) + bl) , where f l(I LR ; W1:l, b1: l) demonstrates the layers receiving the low-resolution image, the original weight and bias being modified by the network as it learns, respectively, and φ(Wl ∗ f l−1 (I LR) + bl) demonstrates the layers applying the weight, which at this point is a 2D tensor of size nl−1✕ nl✕ kl✕ kl where nlé is the number of features in layer l, n0 = C, and kl is the size of the filter in layer l , to the result, feature map, from previous layer and then adding the bias, which at this point are vectors of size nl, and applying the activation function to the result. By convention the second layer applies a convolution of 32 filters and kernel size 3 × 3, followed by a tanh activation layer, and the third layer applies a convolution with fixed number of output channels C ✕ r ✕ r and kernel size 3 × 3.

The fourth, and last, layer is the sub-pixel efficient convolution layer, it can be represented by I SR = f L(I LR) = PS(WL ∗ f L−1 (I LR) + bL), where I SR represents the output image, input, and SR indicates that the image is in super resolution, f L(I LR) demonstrates the layer receiving the image in low resolution and PS(WL ∗ f L−1 (I LR) + bL) demonstrates the layer by applying the weight, which at this point is a 2D tensor of size nL−1✕ nL✕ kL✕ kL where nL is the number of features in the layer L , n0 = C, and kL is the size of the filter in the layer L, to the result, feature map, from the previous layer and then adding the bias, which at this point are vectors of size nL, and applying the pixel shuffle function to the result.

The pixel shuffling function can be represented by PS(T)x,y,c = T ⎣x/r⎦ ,⎣y/r⎦ ,C · r · mod(y,r)+ C · r · mod( x,r)+ c , where PS(T)x,y,c demonstrates the function receiving the result of (WL ∗ f L−1 (I LR) + bL), represented by T, the x and y coordinates of the output pixel in high resolution space and the color channel c , the result of this function being T ⎣x/r⎦ ,⎣y/r⎦ ,C · r · mod(y,r)+ C · r · mod(x,r )+ c , that is, T with base ⎣x/r⎦ , which would be the x coordinate divided by the upscaling ratio r and the result of this applied to a floor function, which rounds the given number down to the nearest integer, ⎣y/r⎦, which would be the y coordinate divided by the upscaling ratio r and the result 33 n n of that applied to the floor function and finally C · r · mod(y, r)+ C · r · mod(x, r)+ c , which would be the number of C color channels multiplied by the upscaling ratio r multiplied by the remainder of y by r , or y % r , plus the number of C color channels mult iplied by the upscaling ratio r multiplied by the remainder of x by r , or x % r , added to the color channel.

The model also proposes a way to train the network, given a set of sample images in high resolution ( I HR , n = 1 . . . N ) for training, the corresponding images in low resolution ( I LR , n = 1 . . . N) should be generated, using filters, and the mean squared error with respect to pixels should be calculated, as an objective function to train the network and measure the difference between the image generated in super resolution and the original image in high resolution .

This function can be represented by l(W1∶L, b1∶L) = 1 ∑r H ∑r W (I HR − f L (I LR))2 , where l(W , b ) demonstrates the error function, r 2HW x=1 x=1 x,y x,y 1∶L 1∶L loss function, which in this case serves as an objective function of decrease, receiving the final weights and biases, the original having already been learned and modified by the network , and 1 ∑r H ∑r W (I HR − f L (I LR))2 demonstrates the application of the error function r 2HW x=1 x=1 x,y x,y mean square, which the formula is MSE = 1 ∑n (Y − Ŷ )2 , where n represents number n i=1 i i of data points, Yi the observed values and Ŷi the predicted values, in the network being n 34 x,y represented by r 2HW , or rH ✕ rW , Yi represented by I HR and Ŷ represented by f L (I LR).
